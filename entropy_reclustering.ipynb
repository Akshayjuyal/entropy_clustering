{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "from datetime import timedelta\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(10)\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='entropy_run.log', mode='w')\n",
    "fhandler.setLevel(logging.DEBUG)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(fhandler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    data_sequence_cluster = []\n",
    "    cluster_sequence_group = defaultdict(list)\n",
    "\n",
    "    \"\"\" pickle file with all rows having format format: \n",
    "            [[seq_name, cluster_number, sequence],.....]\n",
    "    \"\"\"\n",
    "    with open(\"../../data/1k_tags_3k.pkl.npy\",\"rb\") as f:\n",
    "        data_sequence_cluster = np.load(f,allow_pickle=True)\n",
    "\n",
    "    \"\"\"Grouping data with their respective cluster format:\n",
    "            {cluster_number:[[seq_name, cluster_number, sequence],.....],......}\n",
    "    \"\"\"\n",
    "    for row in data_sequence_cluster:\n",
    "        cluster_sequence_group[row[1]].append([row[0],row[2]])\n",
    "    cluster_sequence_group = dict(cluster_sequence_group)\n",
    "\n",
    "    \"\"\"\n",
    "    cluster_info_dict: dictionary with mapping of each sequence to cluster\n",
    "    size_per_cluster: dictionary with mapping of each cluster with no if sequences in it\n",
    "    cluster_df_dict: python list with format: [[seq_name, cluster_number, sequence],.....]\n",
    "    total_seq: total no of sequences read\n",
    "    seq_len: length of each aligned sequence\n",
    "    \"\"\"\n",
    "\n",
    "    cluster_info_dict = dict(pd.read_pickle(\"../../data/closest_haplotypes_500k_trimmed_tn93_before_march_5_0_005\"))\n",
    "    size_per_cluster = Counter(cluster_info_dict.values())\n",
    "    cluster_df_dict=[list(x) for x in data_sequence_cluster]\n",
    "    # total_seq = len(data_sequence_cluster)\n",
    "    seq_len = len(cluster_df_dict[0][2])\n",
    "\n",
    "    return cluster_sequence_group,size_per_cluster,cluster_df_dict, seq_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intial_actg_per_column(seq_len,data,a_c_t_g_counts):\n",
    "\n",
    "    for column in tqdm(range (seq_len)):\n",
    "        # for seq_name in fasta_df[\"sequence_name\"]:\n",
    "        for row in data:\n",
    "            cluster= row[1]\n",
    "            dict_len = len(a_c_t_g_counts[cluster])\n",
    "            if row[2][column]== \"-\":\n",
    "                if  column < dict_len:\n",
    "                    a_c_t_g_counts[cluster][column][\"-\"]+=1\n",
    "                else:\n",
    "                    a_c_t_g_counts[cluster].append({\"A\":0,\"C\":0,\"T\":0,\"G\":0,\"-\":1,\"h_dist\":0,\"entropy\":0})\n",
    "            elif row[2][column]== \"A\":\n",
    "                if column < dict_len:\n",
    "                    a_c_t_g_counts[cluster][column][\"A\"]+=1\n",
    "                else:\n",
    "                    a_c_t_g_counts[cluster].append({\"A\":1,\"C\":0,\"T\":0,\"G\":0,\"-\":0,\"h_dist\":0,\"entropy\":0})\n",
    "            elif row[2][column]== \"C\":\n",
    "                if column < dict_len:\n",
    "                    a_c_t_g_counts[cluster][column][\"C\"]+=1\n",
    "                else:\n",
    "                    a_c_t_g_counts[cluster].append({\"A\":0,\"C\":1,\"T\":0,\"G\":0,\"-\":0,\"h_dist\":0,\"entropy\":0})\n",
    "            elif row[2][column]== \"T\":\n",
    "                if column < dict_len:\n",
    "                    a_c_t_g_counts[cluster][column][\"T\"]+=1\n",
    "                else:\n",
    "                    a_c_t_g_counts[cluster].append({\"A\":0,\"C\":0,\"T\":1,\"G\":0,\"-\":0,\"h_dist\":0,\"entropy\":0})\n",
    "            elif row[2][column]== \"G\":\n",
    "                if column < dict_len:\n",
    "                    a_c_t_g_counts[cluster][column][\"G\"]+=1\n",
    "                else:\n",
    "                    a_c_t_g_counts[cluster].append({\"A\":0,\"C\":0,\"T\":0,\"G\":1,\"-\":0,\"h_dist\":0,\"entropy\":0})\n",
    "    return a_c_t_g_counts\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_initial_hamming_dist_and_entropy_per_column(seq_len,size_per_cluster,a_c_t_g_counts):\n",
    "    # import pdb;pdb.set_trace()\n",
    "    \n",
    "    for column in tqdm(range (seq_len)):\n",
    "        \n",
    "        for cluster in [*size_per_cluster]:\n",
    "            # print((cluster))\n",
    "            entropy = 0\n",
    "            max_actg = max([x[1] for x in a_c_t_g_counts[cluster][column].items() if x[0] in [\"A\",\"C\",\"T\",\"G\"]])\n",
    "            dash = a_c_t_g_counts[cluster][column][\"-\"]\n",
    "            a_c_t_g_counts[cluster][column][\"h_dist\"] = size_per_cluster[cluster]-(max_actg+dash)\n",
    "            # hamming = hamming + a_c_t_g_counts[cluster][column][\"h_dist\"]\n",
    "            \n",
    "            total = sum([x[1] for x in a_c_t_g_counts[cluster][column].items() if x[0] in [\"A\",\"C\",\"T\",\"G\"]])\n",
    "            if (total > 0):\n",
    "                n_a_normalized = a_c_t_g_counts[cluster][column][\"A\"] / total\n",
    "                #print(\"n_a_normalized =\" , n_a_normalized)\n",
    "                n_c_normalized = a_c_t_g_counts[cluster][column][\"C\"] / total\n",
    "                n_t_normalized = a_c_t_g_counts[cluster][column][\"T\"] / total\n",
    "                n_g_normalized = a_c_t_g_counts[cluster][column][\"G\"] / total\n",
    "                entropy_a = n_a_normalized * math.log2(n_a_normalized) if n_a_normalized > 0 else 0\n",
    "                entropy_c = n_c_normalized * math.log2(n_c_normalized) if n_c_normalized > 0 else 0\n",
    "                entropy_g = n_t_normalized * math.log2(n_t_normalized) if n_t_normalized > 0 else 0\n",
    "                entropy_t = n_g_normalized * math.log2(n_g_normalized) if n_g_normalized > 0 else 0\n",
    "                entropy = entropy + entropy_a + entropy_c + entropy_g + entropy_t\n",
    "            try:\n",
    "                a_c_t_g_counts[cluster][column][\"entropy\"] = abs(entropy)\n",
    "            except:\n",
    "                print(entropy)\n",
    "                break\n",
    "        # hamming_entropy_dict[cluster]={\n",
    "        #     \"hamming\":hamming,\n",
    "        #     \"entropy\":entropy\n",
    "        # }\n",
    "    return a_c_t_g_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_initial_hamming_entropy(size_per_cluster, seq_len, a_c_t_g_counts):\n",
    "    hamming_entropy_dict_per_cluster={}\n",
    "    for cluster_id in tqdm([*size_per_cluster]):\n",
    "        entropy, hamming= 0, 0\n",
    "        for val in range(seq_len):\n",
    "            hamming = hamming + a_c_t_g_counts[cluster_id][val][\"h_dist\"]\n",
    "            entropy = entropy + a_c_t_g_counts[cluster_id][val][\"entropy\"]\n",
    "            if hamming < 0:\n",
    "                print(cluster_id,val) \n",
    "                break\n",
    "        hamming_entropy_dict_per_cluster[cluster_id] = {\n",
    "            \"hamming\":hamming,\n",
    "            \"entropy\":entropy\n",
    "        }\n",
    "    return hamming_entropy_dict_per_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_calculations():\n",
    "     cluster_sequence_group,size_per_cluster,cluster_df_dict, seq_len = read_data()\n",
    "\n",
    "     init_a_c_t_g_counts =  dict(zip(list(size_per_cluster.keys()),[[] for val in range(len(size_per_cluster.keys()))]))\n",
    "\n",
    "     intrim_a_c_t_g_counts = intial_actg_per_column(seq_len,cluster_df_dict,init_a_c_t_g_counts)\n",
    "     a_c_t_g_counts = calc_initial_hamming_dist_and_entropy_per_column(seq_len,size_per_cluster,intrim_a_c_t_g_counts)\n",
    "     hamming_entropy_dict_per_cluster = calc_initial_hamming_entropy(size_per_cluster, seq_len, a_c_t_g_counts)\n",
    "    \n",
    "     return hamming_entropy_dict_per_cluster,cluster_sequence_group,a_c_t_g_counts, size_per_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# swapping sequences \"entropy not used\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_entropy_on_swap(old_size_per_cluster, size_per_cluster, new_entropy_per_cluster,old_entropy_per_cluster):\n",
    "    new_en, old_en,total_seq = 0, 0 ,sum(size_per_cluster[key] for key in size_per_cluster.keys())\n",
    "    for cluster_id in [*size_per_cluster]:\n",
    "        new_en +=new_entropy_per_cluster[cluster_id][\"entropy\"]*((size_per_cluster[cluster_id]/total_seq))\n",
    "        old_en +=old_entropy_per_cluster[cluster_id][\"entropy\"] *(( old_size_per_cluster[cluster_id]/total_seq))\n",
    "\n",
    "    return new_en, old_en\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comparing Hamming instead of entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_hamming_on_swap(old_size_per_cluster, size_per_cluster, new_entropy_per_cluster,old_entropy_per_cluster):\n",
    "    new_ham, old_ham,total_seq = 0, 0 ,sum(size_per_cluster[key] for key in size_per_cluster.keys())\n",
    "    for cluster_id in [*size_per_cluster]:\n",
    "        new_ham +=new_entropy_per_cluster[cluster_id][\"hamming\"]*((size_per_cluster[cluster_id]/total_seq))\n",
    "        old_ham +=old_entropy_per_cluster[cluster_id][\"hamming\"] *(( old_size_per_cluster[cluster_id]/total_seq))\n",
    "\n",
    "    return new_ham, old_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_swap_hamming_entropy_per_cluster(seq_len,temp_a_c_t_g_counts,size_per_cluster):\n",
    "    temp_calculations_hamming_entropy={}\n",
    "    for cluster_id in [*size_per_cluster]:\n",
    "        entropy, hamming= 0, 0\n",
    "        for val in range(seq_len):\n",
    "            hamming = hamming + temp_a_c_t_g_counts[cluster_id][val][\"h_dist\"]\n",
    "            entropy = entropy + temp_a_c_t_g_counts[cluster_id][val][\"entropy\"]\n",
    "            if hamming < 0:\n",
    "                print(\"======>\",cluster_id,val)\n",
    "                # break\n",
    "                # pass\n",
    "        \n",
    "        temp_calculations_hamming_entropy[cluster_id] = {\n",
    "            \"hamming\":hamming,\n",
    "            \"entropy\":entropy\n",
    "        }\n",
    "    return temp_calculations_hamming_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_swap_hamming_dist_and_entropy_per_column(seq_len,temp_a_c_t_g_counts, first_r_cluster, second_r_cluster,size_per_cluster):\n",
    "    \n",
    "    for column in range (seq_len):\n",
    "        \n",
    "        for cluster in [first_r_cluster, second_r_cluster]:\n",
    "            # print((cluster))\n",
    "            entropy = 0\n",
    "            max_actg = max([x[1] for x in temp_a_c_t_g_counts[cluster][column].items() if x[0] in [\"A\",\"C\",\"T\",\"G\"]])\n",
    "            dash = temp_a_c_t_g_counts[cluster][column][\"-\"]\n",
    "            temp_a_c_t_g_counts[cluster][column][\"h_dist\"] = size_per_cluster[cluster]-(max_actg+dash)\n",
    "            # hamming = hamming + temp_a_c_t_g_counts[cluster][column][\"h_dist\"]\n",
    "            \n",
    "            total = sum([x[1] for x in temp_a_c_t_g_counts[cluster][column].items() if x[0] in [\"A\",\"C\",\"T\",\"G\"]])\n",
    "            if (total > 0):\n",
    "                n_a_normalized = temp_a_c_t_g_counts[cluster][column][\"A\"] / total\n",
    "                #print(\"n_a_normalized =\" , n_a_normalized)\n",
    "                n_c_normalized = temp_a_c_t_g_counts[cluster][column][\"C\"] / total\n",
    "                n_t_normalized = temp_a_c_t_g_counts[cluster][column][\"T\"] / total\n",
    "                n_g_normalized = temp_a_c_t_g_counts[cluster][column][\"G\"] / total\n",
    "                entropy_a = n_a_normalized * math.log2(n_a_normalized) if n_a_normalized > 0 else 0\n",
    "                entropy_c = n_c_normalized * math.log2(n_c_normalized) if n_c_normalized > 0 else 0\n",
    "                entropy_g = n_t_normalized * math.log2(n_t_normalized) if n_t_normalized > 0 else 0\n",
    "                entropy_t = n_g_normalized * math.log2(n_g_normalized) if n_g_normalized > 0 else 0\n",
    "                entropy = entropy + entropy_a + entropy_c + entropy_g + entropy_t\n",
    "            temp_a_c_t_g_counts[cluster][column][\"entropy\"] = abs(entropy)\n",
    "            \n",
    "    return temp_a_c_t_g_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recount_actg_after_removing(temp_a_c_t_g_counts,sequence, first):\n",
    "    for idx, val in enumerate(sequence):\n",
    "        if val== \"-\":\n",
    "            temp_a_c_t_g_counts[first][idx][\"-\"]-=1\n",
    "        elif val== \"A\":\n",
    "            temp_a_c_t_g_counts[first][idx][\"A\"]-=1\n",
    "        elif val== \"C\":\n",
    "            temp_a_c_t_g_counts[first][idx][\"C\"]-=1\n",
    "        elif val== \"T\":\n",
    "            temp_a_c_t_g_counts[first][idx][\"T\"]-=1\n",
    "        elif val== \"G\":\n",
    "            temp_a_c_t_g_counts[first][idx][\"G\"]-=1\n",
    "    \n",
    "    return temp_a_c_t_g_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recount_actg_after_adding(temp_a_c_t_g_counts,sequence, second):\n",
    "    for idx, val in enumerate(sequence):\n",
    "        if val== \"-\":\n",
    "            temp_a_c_t_g_counts[second][idx][\"-\"]+=1\n",
    "        elif val== \"A\":\n",
    "            temp_a_c_t_g_counts[second][idx][\"A\"]+=1\n",
    "        elif val== \"C\":\n",
    "            temp_a_c_t_g_counts[second][idx][\"C\"]+=1\n",
    "        elif val== \"T\":\n",
    "            temp_a_c_t_g_counts[second][idx][\"T\"]+=1\n",
    "        elif val== \"G\":\n",
    "            temp_a_c_t_g_counts[second][idx][\"G\"]+=1\n",
    "    \n",
    "    return temp_a_c_t_g_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_random_seq_and_cluster(cluster_sequence_group):\n",
    "    \"\"\"Picking Random clusters to swap sequences\"\"\"\n",
    "    all_clusters = list(key for key in cluster_sequence_group.keys() if len(cluster_sequence_group[key]) )\n",
    "    first_r_cluster = random.choice(all_clusters)\n",
    "    all_clusters.remove(first_r_cluster)\n",
    "    second_r_cluster = random.choice(all_clusters)\n",
    "    \"\"\"Picking a random sequnce from first random cluster\"\"\"\n",
    "    random_seq_to_swap = random.choice(range(len(cluster_sequence_group[first_r_cluster])))\n",
    "\n",
    "    return first_r_cluster,second_r_cluster,random_seq_to_swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy_on_swap():\n",
    "    \"\"\"\n",
    "    calculated once and stored\n",
    "\n",
    "    orignal_calculations_hamming_entropy \n",
    "    orignal_clusters\n",
    "    orignal_a_c_t_g_counts\n",
    "\n",
    "    used for comparing swap entropy\n",
    "\n",
    "    temp_calculations_hamming_entropy\n",
    "    temp_cluster_sequence\n",
    "    temp_a_c_t_g_counts\n",
    "\n",
    "    used for saving final data if entropy improves\n",
    "\n",
    "    hamming_entropy_dict_per_cluster\n",
    "    cluster_sequence_group\n",
    "    a_c_t_g_counts\n",
    "\n",
    "    \"\"\"\n",
    "    hamming_entropy_dict_per_cluster,cluster_sequence_group,\\\n",
    "        a_c_t_g_counts,size_per_cluster = get_initial_calculations()\n",
    "\n",
    "    orignal_calculations_hamming_entropy = hamming_entropy_dict_per_cluster\n",
    "    # temp_calculations_hamming_entropy = hamming_entropy_dict_per_cluster\n",
    "    orignal_clusters = cluster_sequence_group\n",
    "    # temp_cluster_sequence = cluster_sequence_group\n",
    "    orignal_a_c_t_g_counts = a_c_t_g_counts\n",
    "    # temp_a_c_t_g_counts = a_c_t_g_counts\n",
    "    orig_size_per_cluster = size_per_cluster\n",
    "    # temp_size_per_cluster = size_per_cluster\n",
    "\n",
    "\n",
    "    \"\"\"swapping Logic\"\"\"\n",
    "    # temp_cluster_sequence = cluster_sequence_group\n",
    "    iteration_info_dict = {}\n",
    "    iteration_count = 1\n",
    "    threshold = 800\n",
    "    iterations_without_change = 0\n",
    "    seq_len = len(cluster_sequence_group[list(cluster_sequence_group.keys())[0]][0][1])\n",
    "    snapshot_duration = 1500\n",
    "    snapshot_counter = 0\n",
    "\n",
    "    start_time = time.process_time()\n",
    "    pbar = tqdm(total=1500)\n",
    "    while(iterations_without_change < threshold):\n",
    "        \n",
    "        temp_cluster_sequence = deepcopy(cluster_sequence_group)\n",
    "        temp_calculations_hamming_entropy =  json.loads(json.dumps(hamming_entropy_dict_per_cluster))\n",
    "        temp_a_c_t_g_counts = json.loads(json.dumps(a_c_t_g_counts))\n",
    "\n",
    "        first_r_cluster,second_r_cluster,random_seq_to_swap = pick_random_seq_and_cluster(temp_cluster_sequence)\n",
    "\n",
    "        sequence = temp_cluster_sequence[first_r_cluster][random_seq_to_swap][1]\n",
    "        sequence_name = temp_cluster_sequence[first_r_cluster][random_seq_to_swap][0]\n",
    "\n",
    "        temp_cluster_sequence[second_r_cluster].append(temp_cluster_sequence[first_r_cluster].pop(random_seq_to_swap))\n",
    "        temp_size_per_cluster = {x:len(temp_cluster_sequence[x]) for x in temp_cluster_sequence.keys()}\n",
    "        size_per_cluster = {x:len(cluster_sequence_group[x]) for x in cluster_sequence_group.keys()}\n",
    "\n",
    "        \"\"\" recounting after swapping \"\"\"\n",
    "        temp_a_c_t_g_counts = recount_actg_after_removing(temp_a_c_t_g_counts,sequence,first_r_cluster)\n",
    "        temp_a_c_t_g_counts = recount_actg_after_adding(temp_a_c_t_g_counts,sequence,second_r_cluster)\n",
    "        temp_a_c_t_g_counts = calc_swap_hamming_dist_and_entropy_per_column(seq_len,temp_a_c_t_g_counts, first_r_cluster, second_r_cluster,temp_size_per_cluster)\n",
    "        temp_calculations_hamming_entropy = cal_swap_hamming_entropy_per_cluster(seq_len,temp_a_c_t_g_counts,temp_size_per_cluster)\n",
    "        \n",
    "        # new_en, old_en = compare_entropy_on_swap(size_per_cluster ,temp_size_per_cluster, temp_calculations_hamming_entropy,hamming_entropy_dict_per_cluster)\n",
    "        \n",
    "        # Using hamming aa a comparison for swapping\n",
    "        new_hamm, old_hamm = compare_hamming_on_swap(size_per_cluster ,temp_size_per_cluster, temp_calculations_hamming_entropy,hamming_entropy_dict_per_cluster)\n",
    "\n",
    "        if (old_hamm-new_hamm)/old_hamm >0.00001:\n",
    "            iterations_without_change = 0\n",
    "            hamming_entropy_dict_per_cluster = json.loads(json.dumps(temp_calculations_hamming_entropy))\n",
    "            cluster_sequence_group = deepcopy(temp_cluster_sequence)\n",
    "            a_c_t_g_counts = json.loads(json.dumps(temp_a_c_t_g_counts))\n",
    "        else:\n",
    "            iterations_without_change+=1\n",
    "            \n",
    "        \n",
    "        iteration_info_dict[iteration_count]= {\n",
    "                                            \"seq_move\":f\"{first_r_cluster} --> {second_r_cluster}\",\n",
    "                                            \"swapped_seq\":sequence_name,\n",
    "                                            \"swap\": \"Failed\" if (old_hamm-new_hamm)/old_hamm >0.00001 else \"Success\",\n",
    "                                            \"old_hamming\": old_hamm,\n",
    "                                            \"new_hamming\": new_hamm,\n",
    "                                            \"hamming_change\": (old_hamm-new_hamm)/old_hamm \n",
    "                                            }\n",
    "        elapsed_time = timedelta(seconds =time.process_time() - start_time)\n",
    "        \n",
    "        pbar.update(iteration_count)\n",
    "        logging.debug(f\"total_moves_made = {iteration_count}, threshold = {threshold}, total_moves_since_last_swap = {iterations_without_change}, elapsed_time: {elapsed_time},hamming({old_hamm, new_hamm})\")\n",
    "        if iteration_count % snapshot_duration == 0:\n",
    "            print(f\"total_moves_made = {iteration_count}, threshold = {threshold}, total_moves_since_last_swap = {iterations_without_change}, \\nelapsed_time: {elapsed_time},hamming({old_hamm, new_hamm})\")\n",
    "            final_funtion(iteration_info_dict,\n",
    "                            a_c_t_g_counts,orignal_a_c_t_g_counts,\n",
    "                            orignal_clusters, cluster_sequence_group,\n",
    "                            orignal_calculations_hamming_entropy, hamming_entropy_dict_per_cluster,\n",
    "                            snapshot_counter = snapshot_counter\n",
    "                            )\n",
    "            snapshot_counter+=1\n",
    "        iteration_count+=1\n",
    "    logging.debug(\"complete\")\n",
    "    print(\"complete\")\n",
    "    final_funtion(iteration_info_dict,\n",
    "                            a_c_t_g_counts,orignal_a_c_t_g_counts,\n",
    "                            orignal_clusters, cluster_sequence_group,\n",
    "                            orignal_calculations_hamming_entropy, hamming_entropy_dict_per_cluster\n",
    "                            )\n",
    "\n",
    "    # return iteration_info_dict\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_funtion(iteration_info_dict,\n",
    "                    a_c_t_g_counts,orignal_a_c_t_g_counts,\n",
    "                    orignal_clusters, cluster_sequence_group,\n",
    "                    orignal_calculations_hamming_entropy, hamming_entropy_dict_per_cluster, snapshot_counter = -20):\n",
    "    \n",
    "    orignal_clust = {y[0]:x for x in [*orignal_clusters] for y in orignal_clusters[x]}\n",
    "    new_clust = {y[0]:x for x in [*cluster_sequence_group] for y in cluster_sequence_group[x]}\n",
    "    if not snapshot_counter ==-20:\n",
    "        path = f\"../../data/monte_carlo/multiple_runs_hamming/run1/{snapshot_counter}\"\n",
    "        if not os.path.exists(f\"../../data/monte_carlo/multiple_runs_hamming/run1/{snapshot_counter}\"):\n",
    "            os.mkdir(path)  \n",
    "    else:\n",
    "        path = f\"../../data/monte_carlo/multiple_runs_hamming/run1\"\n",
    "\n",
    "\n",
    "    with open (f\"{path}/iteration_info.json\",\"w\") as iid:\n",
    "        json.dump(iteration_info_dict,iid)\n",
    "    pd.DataFrame.from_dict(iteration_info_dict, orient='index').to_csv(f\"{path}/iteration_info.csv\",index=False)\n",
    "    \n",
    "    with open (f\"{path}/new_actg_counts.json\",\"w\") as actg:\n",
    "        json.dump(a_c_t_g_counts,actg)\n",
    "    with open (f\"{path}/new_clustering.json\",\"w\") as nc:\n",
    "        json.dump(new_clust,nc)\n",
    "    \n",
    "    with open (f\"{path}/new_ent_ham_per_cluster.json\",\"w\") as neh:\n",
    "        json.dump(hamming_entropy_dict_per_cluster,neh)\n",
    "    \n",
    "    if snapshot_counter ==-20:\n",
    "        with open (f\"{path}/orig_actg_counts.json\",\"w\") as oactg:\n",
    "            json.dump(orignal_a_c_t_g_counts,oactg)\n",
    "\n",
    "        with open (f\"{path}/orignal_clustering.json\",\"w\") as oc:\n",
    "            json.dump(orignal_clust,oc)\n",
    "\n",
    "        with open (f\"{path}/orignal_ent_ham_per_cluster.json\",\"w\") as oeh:\n",
    "            json.dump(orignal_calculations_hamming_entropy,oeh)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 944.16it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3550.96it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 1508.45it/s]\n",
      "240166486it [10:11:37, 6544.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 1500, threshold = 800, total_moves_since_last_swap = 3, \n",
      "elapsed_time: 0:03:58.695655,hamming((504.6963123644252, 504.83568329718))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 3000, threshold = 800, total_moves_since_last_swap = 0, \n",
      "elapsed_time: 0:07:49.466618,hamming((461.07239696312354, 461.03280911062893))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 4500, threshold = 800, total_moves_since_last_swap = 0, \n",
      "elapsed_time: 0:11:36.936929,hamming((421.8698481561822, 421.6152386117136))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 6000, threshold = 800, total_moves_since_last_swap = 2, \n",
      "elapsed_time: 0:15:23.400338,hamming((392.77711496746196, 393.0008134490238))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 7500, threshold = 800, total_moves_since_last_swap = 7, \n",
      "elapsed_time: 0:19:10.884525,hamming((366.40997830802604, 366.4796637744035))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 9000, threshold = 800, total_moves_since_last_swap = 14, \n",
      "elapsed_time: 0:22:58.663383,hamming((347.6268980477223, 347.7879609544468))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 10500, threshold = 800, total_moves_since_last_swap = 5, \n",
      "elapsed_time: 0:26:42.531978,hamming((334.33866594360086, 334.396420824295))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 12000, threshold = 800, total_moves_since_last_swap = 1, \n",
      "elapsed_time: 0:30:24.093706,hamming((322.3988611713666, 322.537147505423))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 13500, threshold = 800, total_moves_since_last_swap = 5, \n",
      "elapsed_time: 0:34:06.726543,hamming((309.8039587852495, 309.86632321041213))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 15000, threshold = 800, total_moves_since_last_swap = 7, \n",
      "elapsed_time: 0:37:44.686756,hamming((301.89208242950116, 302.02603036876366))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 16500, threshold = 800, total_moves_since_last_swap = 5, \n",
      "elapsed_time: 0:41:25.544277,hamming((294.62066160520607, 294.6957700650759))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 18000, threshold = 800, total_moves_since_last_swap = 14, \n",
      "elapsed_time: 0:45:05.158583,hamming((288.2711496746204, 288.37662689804773))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 19500, threshold = 800, total_moves_since_last_swap = 1, \n",
      "elapsed_time: 0:48:47.200473,hamming((282.10330802603045, 282.4482104121476))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 21000, threshold = 800, total_moves_since_last_swap = 13, \n",
      "elapsed_time: 0:52:22.517193,hamming((278.6317787418655, 278.76572668112794))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 22500, threshold = 800, total_moves_since_last_swap = 4, \n",
      "elapsed_time: 0:56:03.398577,hamming((275.63855748373106, 276.04663774403474))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 24000, threshold = 800, total_moves_since_last_swap = 31, \n",
      "elapsed_time: 0:59:41.484542,hamming((271.60168112798266, 271.7494577006508))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 25500, threshold = 800, total_moves_since_last_swap = 22, \n",
      "elapsed_time: 1:03:19.265147,hamming((269.2133947939262, 269.3367678958785))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 27000, threshold = 800, total_moves_since_last_swap = 2, \n",
      "elapsed_time: 1:06:54.831631,hamming((266.8039587852494, 266.93085683297176))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 28500, threshold = 800, total_moves_since_last_swap = 10, \n",
      "elapsed_time: 1:10:29.306368,hamming((264.0284707158351, 264.1743492407809))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 30000, threshold = 800, total_moves_since_last_swap = 11, \n",
      "elapsed_time: 1:14:06.408921,hamming((262.0566702819956, 262.51979392624725))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 31500, threshold = 800, total_moves_since_last_swap = 60, \n",
      "elapsed_time: 1:17:48.036658,hamming((259.915672451193, 260.11415401301514))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 33000, threshold = 800, total_moves_since_last_swap = 55, \n",
      "elapsed_time: 1:21:27.520466,hamming((258.2966377440347, 258.73454446854663))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 34500, threshold = 800, total_moves_since_last_swap = 53, \n",
      "elapsed_time: 1:25:05.992959,hamming((256.80667028199565, 256.9777657266811))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 36000, threshold = 800, total_moves_since_last_swap = 16, \n",
      "elapsed_time: 1:28:43.006496,hamming((255.5252169197397, 255.5835140997831))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 37500, threshold = 800, total_moves_since_last_swap = 32, \n",
      "elapsed_time: 1:32:22.151981,hamming((253.9029284164859, 253.98047722342736))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 39000, threshold = 800, total_moves_since_last_swap = 4, \n",
      "elapsed_time: 1:36:04.137042,hamming((252.41811279826464, 252.6870932754881))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 40500, threshold = 800, total_moves_since_last_swap = 26, \n",
      "elapsed_time: 1:39:42.002643,hamming((250.91675704989157, 251.08893709327552))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 42000, threshold = 800, total_moves_since_last_swap = 169, \n",
      "elapsed_time: 1:43:20.325395,hamming((250.15292841648585, 250.47505422993487))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 43500, threshold = 800, total_moves_since_last_swap = 12, \n",
      "elapsed_time: 1:46:58.409255,hamming((249.1098156182213, 249.17543383947944))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 45000, threshold = 800, total_moves_since_last_swap = 56, \n",
      "elapsed_time: 1:50:35.240063,hamming((247.941431670282, 248.05260303687635))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 46500, threshold = 800, total_moves_since_last_swap = 40, \n",
      "elapsed_time: 1:54:11.342703,hamming((247.2513557483731, 247.78362255965294))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 48000, threshold = 800, total_moves_since_last_swap = 80, \n",
      "elapsed_time: 1:57:53.128909,hamming((246.09951193058566, 246.31046637744035))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 49500, threshold = 800, total_moves_since_last_swap = 42, \n",
      "elapsed_time: 2:01:31.564726,hamming((245.3394793926248, 245.46637744034715))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 51000, threshold = 800, total_moves_since_last_swap = 20, \n",
      "elapsed_time: 2:05:09.699248,hamming((244.70715835140993, 245.2188177874186))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 52500, threshold = 800, total_moves_since_last_swap = 12, \n",
      "elapsed_time: 2:08:52.013286,hamming((243.98156182212585, 244.09219088937095))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 54000, threshold = 800, total_moves_since_last_swap = 188, \n",
      "elapsed_time: 2:12:33.373960,hamming((243.12960954446856, 243.3785249457701))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 55500, threshold = 800, total_moves_since_last_swap = 22, \n",
      "elapsed_time: 2:16:16.237052,hamming((242.40455531453364, 242.4796637744035))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 57000, threshold = 800, total_moves_since_last_swap = 12, \n",
      "elapsed_time: 2:19:59.224454,hamming((241.77223427331887, 241.96963123644255))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 58500, threshold = 800, total_moves_since_last_swap = 42, \n",
      "elapsed_time: 2:23:43.017022,hamming((241.22749457700652, 241.30748373101957))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 60000, threshold = 800, total_moves_since_last_swap = 1, \n",
      "elapsed_time: 2:27:23.612567,hamming((240.5875813449024, 240.75298264642083))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 61500, threshold = 800, total_moves_since_last_swap = 77, \n",
      "elapsed_time: 2:31:00.648086,hamming((240.27847071583514, 240.30558568329718))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 63000, threshold = 800, total_moves_since_last_swap = 59, \n",
      "elapsed_time: 2:34:38.651252,hamming((239.86117136659433, 240.17624728850325))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 64500, threshold = 800, total_moves_since_last_swap = 17, \n",
      "elapsed_time: 2:38:20.116323,hamming((239.2947396963124, 239.31697396963128))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 66000, threshold = 800, total_moves_since_last_swap = 28, \n",
      "elapsed_time: 2:42:04.856940,hamming((238.59951193058572, 238.90753796095447))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 67500, threshold = 800, total_moves_since_last_swap = 91, \n",
      "elapsed_time: 2:45:50.325718,hamming((238.1247288503254, 238.5385032537961))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 69000, threshold = 800, total_moves_since_last_swap = 103, \n",
      "elapsed_time: 2:49:33.091939,hamming((237.73942516268983, 237.83947939262475))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 70500, threshold = 800, total_moves_since_last_swap = 4, \n",
      "elapsed_time: 2:53:18.658841,hamming((236.91404555314531, 237.2632863340564))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 72000, threshold = 800, total_moves_since_last_swap = 152, \n",
      "elapsed_time: 2:57:04.421561,hamming((236.40618221258134, 236.55585683297178))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 73500, threshold = 800, total_moves_since_last_swap = 10, \n",
      "elapsed_time: 3:00:49.028308,hamming((235.8215835140998, 235.96881778741863))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 75000, threshold = 800, total_moves_since_last_swap = 101, \n",
      "elapsed_time: 3:04:34.030976,hamming((235.19034707158357, 235.222885032538))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 76500, threshold = 800, total_moves_since_last_swap = 24, \n",
      "elapsed_time: 3:08:16.581502,hamming((234.4604121475054, 234.62906724511927))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 78000, threshold = 800, total_moves_since_last_swap = 19, \n",
      "elapsed_time: 3:11:59.773206,hamming((233.95797180043385, 234.30965292841648))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 79500, threshold = 800, total_moves_since_last_swap = 32, \n",
      "elapsed_time: 3:15:41.089010,hamming((233.45471800433842, 233.68329718004344))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 81000, threshold = 800, total_moves_since_last_swap = 61, \n",
      "elapsed_time: 3:19:23.058624,hamming((233.22640997830808, 233.34137744034712))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 82500, threshold = 800, total_moves_since_last_swap = 103, \n",
      "elapsed_time: 3:23:05.460742,hamming((233.01409978308033, 233.04473969631243))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 84000, threshold = 800, total_moves_since_last_swap = 15, \n",
      "elapsed_time: 3:26:50.007387,hamming((232.67814533622558, 232.7475596529284))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 85500, threshold = 800, total_moves_since_last_swap = 33, \n",
      "elapsed_time: 3:30:36.005969,hamming((232.20227765726682, 232.27575921908894))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 87000, threshold = 800, total_moves_since_last_swap = 105, \n",
      "elapsed_time: 3:34:20.837659,hamming((232.05178958785248, 232.20715835140996))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 88500, threshold = 800, total_moves_since_last_swap = 7, \n",
      "elapsed_time: 3:38:09.274214,hamming((231.6423535791757, 231.81073752711492))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 90000, threshold = 800, total_moves_since_last_swap = 21, \n",
      "elapsed_time: 3:41:59.005840,hamming((231.20444685466376, 231.55721258134488))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 91500, threshold = 800, total_moves_since_last_swap = 5, \n",
      "elapsed_time: 3:45:54.829044,hamming((230.8519522776573, 230.93383947939265))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 93000, threshold = 800, total_moves_since_last_swap = 23, \n",
      "elapsed_time: 3:49:45.025676,hamming((230.4349240780911, 230.5249457700651))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 94500, threshold = 800, total_moves_since_last_swap = 26, \n",
      "elapsed_time: 3:53:32.545555,hamming((230.20634490238618, 230.48996746203906))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 96000, threshold = 800, total_moves_since_last_swap = 78, \n",
      "elapsed_time: 3:57:20.312774,hamming((230.01599783080258, 230.2429501084598))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 97500, threshold = 800, total_moves_since_last_swap = 104, \n",
      "elapsed_time: 4:01:07.524726,hamming((229.7781995661605, 229.95010845986982))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 99000, threshold = 800, total_moves_since_last_swap = 2, \n",
      "elapsed_time: 4:04:54.676940,hamming((229.24511930585683, 229.5452819956616))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 100500, threshold = 800, total_moves_since_last_swap = 56, \n",
      "elapsed_time: 4:08:44.450041,hamming((229.07809110629066, 229.16784164858998))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 102000, threshold = 800, total_moves_since_last_swap = 150, \n",
      "elapsed_time: 4:12:29.337297,hamming((228.79528199566158, 229.1423535791757))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 103500, threshold = 800, total_moves_since_last_swap = 67, \n",
      "elapsed_time: 4:16:19.533044,hamming((228.41594360086762, 228.61632321041205))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 105000, threshold = 800, total_moves_since_last_swap = 66, \n",
      "elapsed_time: 4:20:09.099346,hamming((228.23861171366593, 228.27630151843817))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 106500, threshold = 800, total_moves_since_last_swap = 135, \n",
      "elapsed_time: 4:23:54.831823,hamming((228.02765726681127, 228.20797180043382))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 108000, threshold = 800, total_moves_since_last_swap = 36, \n",
      "elapsed_time: 4:27:43.288611,hamming((227.62527114967463, 227.92895878524948))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 109500, threshold = 800, total_moves_since_last_swap = 127, \n",
      "elapsed_time: 4:31:30.896005,hamming((227.1046637744035, 227.38693058568327))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 111000, threshold = 800, total_moves_since_last_swap = 90, \n",
      "elapsed_time: 4:35:19.210649,hamming((226.74972885032537, 226.7516268980477))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 112500, threshold = 800, total_moves_since_last_swap = 17, \n",
      "elapsed_time: 4:39:04.088299,hamming((226.4907809110629, 226.78416485900217))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 114000, threshold = 800, total_moves_since_last_swap = 240, \n",
      "elapsed_time: 4:42:52.822945,hamming((226.17218004338395, 226.53226681127984))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 115500, threshold = 800, total_moves_since_last_swap = 54, \n",
      "elapsed_time: 4:46:39.681387,hamming((225.9194685466378, 226.2028199566161))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 117000, threshold = 800, total_moves_since_last_swap = 55, \n",
      "elapsed_time: 4:50:27.148286,hamming((225.77874186550977, 225.88720173535793))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 118500, threshold = 800, total_moves_since_last_swap = 97, \n",
      "elapsed_time: 4:54:19.426422,hamming((225.62201735357922, 225.83270065075925))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 120000, threshold = 800, total_moves_since_last_swap = 73, \n",
      "elapsed_time: 4:58:10.302241,hamming((225.42326464208244, 225.50271149674623))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 121500, threshold = 800, total_moves_since_last_swap = 229, \n",
      "elapsed_time: 5:02:00.949411,hamming((225.09436008676786, 225.39777657266808))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_moves_made = 123000, threshold = 800, total_moves_since_last_swap = 637, \n",
      "elapsed_time: 5:05:52.333308,hamming((225.08188720173536, 225.36469631236443))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "calculate_entropy_on_swap()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fba1ad6f3fae8c28c590ddc4333277e6681be5e051428e98b7d7639a0269d1f9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('pypy': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
